Stable Diffusion is a text-to-image generation model based on latent diffusion. By progressively adding and removing noise in latent space, it turns random noise into images that match a text prompt.
In recent years, Stable Diffusion has continued to evolve, with many optimized variants emerging in the open-source community, significantly improving generation quality, speed, and computational efficiency.
This demo uses **Stable Diffusion LCM Dreamshaper V7**, a lightweight variant that combines Latent Consistency Model (LCM) acceleration to maintain high-quality image generation with very few inference steps, producing clear images in just **4 steps**.
This guide walks through how to deploy the model to the NPU on Rockchip chips using the RKNN toolchain, enabling efficient, low-latency on-device image generation.

:::tip
This document uses RK3588 and Dreamshaper V7 as an example to demonstrate how to deploy a text-to-image model on the Rockchip NPU.
You need to set up the RKNN-related environment on your PC in advance.
For detailed environment setup steps, refer to [RKNN Installation](./rknn_install).
:::

## Download model files

**Radxa provides converted RKNN models and executables with an output resolution of 256x256. You can download and use them directly by following the steps below.**

- **Download model files using modelscope**

  - **Create a directory to store the model files**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
mkdir sd-lcm-rknn && cd sd-lcm-rknn
```

</NewCodeBlock>

- **Install the modelscope package via pip**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
# Use a recent Python version to avoid compatibility issues
pip3 install modelscope
```

</NewCodeBlock>

- **Download the Stable-Diffusion-LCM_RKNN repository**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
modelscope download --model radxa/Stable-Diffusion-LCM_RKNN
```

</NewCodeBlock>

## (Optional) Convert the model

**If you want to set a custom output resolution, you can convert the model yourself. Follow the steps below.**

- **Download the ONNX model from HuggingFace and convert it to an RKNN model**

  - **Create a directory to store the model files**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
mkdir sd-lcm-rknn && cd sd-lcm-rknn
```

</NewCodeBlock>

- **Clone the model repository**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
# Install git lfs if it is not already available
git lfs install
git clone https://huggingface.co/thanhtantran/Stable-Diffusion-1.5-LCM-ONNX-RKNN2
```

</NewCodeBlock>

- **Activate the virtual environment**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
conda activate your_rknn_env
```

</NewCodeBlock>

- **Optionally run _run_onnx-lcm.py_ to validate the model**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
python run_onnx-lcm.py -i ./model -o ./images --prompt "Majestic mountain landscape with snow-capped peaks, autumn foliage in vibrant reds and oranges, a turquoise river winding through a valley, crisp and serene atmosphere, ultra-realistic style."
```

</NewCodeBlock>

- **Run _convert-onnx-to-rknn.py_ to convert the model**

<NewCodeBlock tip="Linux PC" type="PC">

```bash
python convert-onnx-to-rknn.py -i ./model -r NxN
```

</NewCodeBlock>

- **Organize the files into the following directory structure, then proceed to the next step**

```txt
---sd-lcm-rknn
     ---model
         ---scheduler
         ---scheduler_config.json
     ---text_encoder
         ---config.json
         ---model.rknn
     ---unet
         ---config.json
         ---model.rknn
     ---vae_decoder
         ---config.json
         ---model.rknn
     ---run_rknn-lcm.py
```

## Deploy on the device

- **Copy the converted RKNN models and executables to the device**

  - **Enter the corresponding directory on the device**

<NewCodeBlock tip="Radxa SBC" type="device">

```bash
cd sd-lcm-rknn
```

</NewCodeBlock>

- **Create a Python virtual environment**

<NewCodeBlock tip="Radxa SBC" type="device">

```bash
python -m venv .venv
```

</NewCodeBlock>

- **Activate the virtual environment**

<NewCodeBlock tip="Radxa SBC" type="device">

```bash
source .venv/bin/activate
```

</NewCodeBlock>

- **Install dependencies**

<NewCodeBlock tip="Radxa SBC" type="device">

```bash
pip3 install diffusers pillow "numpy<2.0" torch transformers rknn-toolkit-lite2
```

</NewCodeBlock>

- **Run the script**

<NewCodeBlock tip="Radxa SBC" type="device">

```bash
# Use -h to view help. If you converted the model yourself, update the resolution argument accordingly.
python ./run_rknn-lcm.py -i ./model -o ./images -s 256x256 --prompt "Majestic mountain landscape with snow-capped peaks, autumn foliage in vibrant reds and oranges, a turquoise river winding through a valley, crisp and serene atmosphere, ultra-realistic style."
```

</NewCodeBlock>

## Results and performance

- **256x256 output image on the device**

![sb-lcm-mountain.webp](/img/rock5b/sb-lcm-mountain.webp)

- **Single-run performance (for reference only):**

```txt
Text encoder load time: Took 0.7 seconds.
UNet load time: Took 2.8 seconds.
VAE decoder load time: Took 0.4 seconds.
Prompt encoding time: 0.08s
Inference time: 4.55s
Decode time: 3.15s
Total time: 7.78s
```
