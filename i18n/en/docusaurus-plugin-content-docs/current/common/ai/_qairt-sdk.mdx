The Radxa Dragon series SoC is equipped with the Qualcomm® Hexagon™ Processor (NPU), a hardware accelerator specifically designed for AI inference.
To utilize the NPU for model inference, you need to use the QAIRT (Qualcomm® AI Runtime) SDK to port pre-trained models.
Qualcomm® provides a series of SDKs to help developers port their AI models to the NPU.

- Model Quantization Library: [AIMET](#aimet)

- Model Porting SDK: [QAIRT](#qairt)

- Model Application Library: [QAI-APP-BUILDER](#qai-appbuilder)

- Online Model Conversion Library: [QAI-HUB](#qai-hub)

## Qualcomm® NPU Software Stack

### QAIRT

QAIRT (Qualcomm® AI Runtime) SDK is a software package that integrates Qualcomm® AI software products,
including Qualcomm® AI Engine Direct, Qualcomm® Neural Processing SDK, and Qualcomm® Genie.
QAIRT provides developers with all the necessary tools for porting and deploying AI models on Qualcomm® hardware accelerators,
as well as the runtime for running models on CPU, GPU, and NPU.

#### Supported Inference Backends

- CPU

- GPU

- NPU

<div style={{ textAlign: "center" }}>
  <img src="/en/img/dragon/q6a/qairt_arch.webp" style={{ width: "65%" }} />
  QAIRT SDK Architecture
</div>

#### QAIRT Model Formats

QAIRT supports the following 3 model file formats based on different systems and inference backends:

| Format             | Backend         | Cross-OS | Cross-Chip |
| ------------------ | --------------- | -------- | ---------- |
| Library            | CPU / GPU / NPU | No       | Yes        |
| DLC                | CPU / GPU / NPU | Yes      | Yes        |
| **Context Binary** | **NPU**         | Yes      | No         |

:::tip
This document focuses on model porting and deployment using the NPU, specifically covering the **Context-Binary** format which offers optimal memory usage and performance.
For information on converting other model formats and inference methods for different backends, please refer to the [**QAIRT SDK Documentation**](qairt-install#complete-sdk-documentation)
:::

#### SoC Architecture Reference Table

| SoC     | dsp_arch | soc_id |
| ------- | -------- | ------ |
| QCS6490 | v68      | 35     |
| QCS9075 | v73      | 77     |

{/* ## TODO */}

#### Documentation

- [**QAIRT SDK Installation**](./qairt-install)

- [**QAIRT SDK Usage Examples**](./qairt-usage)

- [**NPU Quick Start**](./quick-example)

- [**Complete QAIRT Documentation**](./qairt-install)

### AIMET

[**AIMET**](https://github.com/quic/aimet) (AI Model Efficiency Toolkit) is a quantization tool for deep learning models (such as PyTorch and ONNX). AIMET enhances the performance of deep learning models by reducing computational load and memory usage.
With AIMET, developers can quickly iterate to find the optimal quantization configuration, achieving the best balance between accuracy and latency. Developers can compile and deploy quantized models exported from AIMET on Qualcomm NPUs using [QAIRT](./qairt-usage), or run them directly with ONNX-Runtime.

<div style={{ textAlign: "center" }}>
  <img src="/en/img/dragon/q6a/aimet_overview.webp" style={{ width: "100%" }} />
  AIMET OVERVIEW
</div>

#### Documentation

- [**AIMET Quantization Tool**](./aimet)

- [**Complete AIMET Documentation**](https://quic.github.io/aimet-pages/releases/latest/index.html#)

- [**AIMET Repository**](https://github.com/quic/aimet)

### QAI-APPBUILDER

[**Quick AI Application Builder**](https://github.com/quic/ai-engine-direct-helper) (QAI AppBuilder) helps developers easily use the [Qualcomm® AI Runtime SDK](qairt-sdk#qairt) to deploy AI models and design AI applications on Qualcomm® SoC platforms equipped with the Qualcomm® Hexagon™ Processor (NPU).
It encapsulates the model deployment APIs into a set of simplified interfaces for loading models onto the NPU and performing inference. QAI AppBuilder significantly reduces the complexity of model deployment for developers and provides multiple demos as references for designing their own AI applications.

<div style={{ textAlign: "center" }}>
  <img
    src="/en/img/dragon/q6a/qai_app_builder_1.webp"
    style={{ width: "65%" }}
  />
  QAI-APPBUILDER Architecture
</div>

#### Documentation

- [**QAI AppBuilder**](./qai-appbuilder)

- [**QAI AppBuilder Repository**](https://github.com/quic/ai-engine-direct-helper)

### QAI-Hub

[**Qualcomm® AI Hub**](https://aihub.qualcomm.com/) (QAI-Hub) is a one-stop cloud platform for model conversion, offering online model compilation, quantization, performance analysis, inference, and download services.
Qualcomm® AI Hub automates the model conversion process from pre-trained models to device runtimes, automatically configuring devices in the cloud for performance analysis and inference.
The [**Qualcomm® AI Hub Models**](https://github.com/quic/ai-hub-models) (QAI-Hub-Models) project leverages the cloud services provided by [QAI-Hub](https://app.aihub.qualcomm.com/docs/index.html), supporting command-line based **quantization, compilation, inference, analysis, and download** of models from the [model list](qai-hub-models#model-list) on cloud devices.

<div style={{ textAlign: "center" }}>
  <img src="/en/img/dragon/q6a/qai-hub.webp" style={{ width: "100%" }} />
  QAI-Hub WORKFLOW
</div>

#### Documentation

- [**Qualcomm® AI Hub**](https://app.aihub.qualcomm.com/docs/index.html)

- [**Using Qualcomm® AI Hub Models**](qai-hub-models)

- [**Qualcomm® AI Hub Models Repository**](https://github.com/quic/ai-hub-models)
