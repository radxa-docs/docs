QAIRT provides developers with all the necessary tools for porting and deploying AI models on Qualcomm® hardware accelerators.
This document details the complete process of using QAIRT to port the [resnet50](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) object recognition model,
using NPU inference on a board running Ubuntu as an example, to fully explain the method of porting models to NPU hardware using the QAIRT SDK.

## Model Porting Workflow

<div style={{ textAlign: "center" }}>
  <img src="/en/img/dragon/q6a/qairt_workflow.webp" style={{ width: "65%" }} />
  QAIRT WORKFLOW
</div>

## NPU Model Porting Steps

To port mainstream AI model frameworks (PyTorch, TensorFlow, TFLite, ONNX) to Qualcomm NPU for hardware-accelerated inference, the model needs to be converted to the `Context-Binary` format specific to Qualcomm NPU. The following steps are required to convert to the Context-Binary format:

1. Prepare a pre-trained floating-point model
2. Use [AIMET](https://github.com/quic/aimet) for efficient model optimization and quantization of the pre-trained model (optional)
3. Convert the model to floating-point DLC format using QAIRT tools
4. Quantize the floating-point DLC model using QAIRT tools
5. Convert the DLC model to Context-Binary format using QAIRT tools
6. Perform NPU inference on the Context-Binary model using QAIRT tools

## Model Conversion Example

### Set Up QAIRT Development Environment

:::tip

- Refer to [**QAIRT SDK Installation**](./qairt-install) to set up the QAIRT working environment
  :::

### Prepare Pre-trained Model

Clone the resnet50 example repository in the QAIRT SDK

<NewCodeBlock tip="X86 Linux PC" type="PC">

```bash
cd qairt/2.37.1.250807/examples/Models/
git clone https://github.com/ZIFENG278/resnet50_qairt_example.git && cd resnet50_qairt_example
```

</NewCodeBlock>

Using PyTorch [resnet50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) as an example,
export an ONNX model with input shape (batch_size,3,224,224). Please use the following script to export the model:

<NewCodeBlock tip="X86 Linux PC" type="PC">

```bash
python3 export_onnx.py
```

</NewCodeBlock>

The exported ONNX model is saved as `resnet50.onnx`

### Model Quantization with AIMET (Optional)

[AIMET](https://github.com/quic/aimet) is an independent open-source model quantization library not included in the QAIRT SDK. Before porting the model,
it is recommended to use AIMET for model optimization and quantization of pre-trained models, which can maximize the model's inference performance while maintaining its original accuracy.

:::tip

Using AIMET for quantization doesn't conflict with QAIRT's quantization. AIMET provides advanced quantization, while QAIRT's quantization is standard linear quantization.

- For AIMET installation, refer to [**AIMET Quantization Tool**](aimet)
- For AIMET usage, refer to [**AIMET Usage Examples**](aimet#aimet-usage-examples)
  :::

### DLC Model Conversion

Using the `qairt-converter` in the QAIRT SDK, you can convert models from Onnx/TensorFlow/TFLite/PyTorch frameworks and [AIMET](aimet) output files into DLC (Deep Learning Container) model files.
`qairt-converter` automatically recognizes the model framework based on the file extension.

:::tip

The following sections are divided into porting steps for models optimized and quantized with AIMET and original ONNX model files. Please note the distinction.
:::

<Tabs>
    <TabItem value="AIMET">

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    qairt-converter --input_network ./aimet_quant/resnet50.onnx --quantization_overrides ./aimet_quant/resnet50.encodings --output_path resnet50.dlc -d 'input' 1,3,224,224
    ```

    </NewCodeBlock>

    `qairt-converter` will generate a quantized DLC model file saved as `resnet50_aimet.dlc`

    </TabItem>
    <TabItem value="Pretrained ONNX">

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    qairt-converter --input_network ./resnet50.onnx -d 'input' 1,3,224,224
    ```

    </NewCodeBlock>


    `qairt-converter` will generate a floating-point DLC model file saved as `resnet50.dlc`

    </TabItem>

</Tabs>

:::tip
This DLC file can be used for CPU/GPU inference using the Qualcomm® Neural Processing SDK API. For details, please refer to the [SNPE Documentation](qairt-install#sdk-documentation-location)
:::

:::tip
For more information on using `qairt-converter`, please refer to [**qairt-converter**](qairt-tools#qairt-converter)
:::

### Quantizing DLC Models

:::tip
DLC models obtained from AIMET models through `qairt-converter` are already quantized. You can skip this quantization step when using AIMET models.
:::

NPU only supports quantized models. Before converting to the `Context-Binary` format, the floating-point DLC model needs to be quantized.
QAIRT provides a quantization tool `qairt-quantizer` that can quantize DLC models to INT8/INT16 types using quantization algorithms.

#### Prepare Calibration Dataset

The `create_resnet50_raws.py` script in the `scripts` directory can create raw format files for resnet50 model input to be used as quantization input

<NewCodeBlock tip="X86 Linux PC" type="PC">

```bash
cd scripts
python3 create_resnet50_raws.py --dest ../data/calibration/crop --img_folder ../data/calibration/ --size 224
```

</NewCodeBlock>

#### Prepare Calibration File List

The `create_file_list.py` script in the `scripts` directory can create a file list for model quantization calibration

<NewCodeBlock tip="X86 Linux PC" type="PC">

```bash
cd scripts
python3 create_file_list.py --input_dir ../data/calibration/crop/ --output_filename ../model/calib_list.txt -e *.raw
```

</NewCodeBlock>

The generated `calib_list.txt` contains the absolute paths to the calibration raw files

#### Perform DLC Model Quantization

<NewCodeBlock tip="X86 Linux PC" type="PC">

```bash
cd model
qairt-quantizer --input_dlc ./resnet50.dlc --input_list ./calib_list.txt  --output_dlc resnet50_quantized.dlc
```

</NewCodeBlock>

The target quantized model is saved as `resnet50_quantized.dlc`

:::tip
For more information on using `qairt-quantizer`, please refer to [**qairt-quantizer**](qairt-tools#qairt-quantizer)
:::

### Generate Context-Binary Model

Before using the quantized DLC model for NPU inference, the DLC model needs to be converted to the Context-Binary format. The purpose of this conversion is to prepare the instructions for running the DLC model graph on the target hardware in advance on the host.
This enables inference on the NPU and reduces the model's initialization time and memory consumption on the board.

Use the `qnn-context-binary-generator` in the QAIRT SDK to convert the quantized DLC format model to the Context-Binary format model.

#### Create Model Conversion Config Files

Since hardware-specific optimization is performed on the x86 host, two config files need to be created here

**SoC Architecture Reference Table**

| SoC     | dsp_arch | soc_id |
| ------- | -------- | ------ |
| QCS6490 | v68      | 35     |
| QCS9075 | v73      | 77     |

- config_backend.json

      <Tabs>

  <TabItem value="QCS6490">

          Please select the appropriate dsp_arch and soc_id based on your SoC NPU architecture. Here we use QCS6490 SoC as an example

          <NewCodeBlock tip="X86 Linux PC" type="PC">

          ```bash
          vim config_backend.json
          ```

          </NewCodeBlock>

          ```vim
          {
              "graphs": [
                  {
                      "graph_names": [
                          "resnet50"
                      ],
                      "vtcm_mb": 0
                  }
              ],
              "devices": [
                  {
                      "dsp_arch": "v68",
                      "soc_id": 35
                  }
              ]
          }
          ```


          </TabItem>

          <TabItem value="QCS9075">

          Please select the appropriate dsp_arch and soc_id based on your SoC NPU architecture. Here we use QCS9075 SoC as an example

          <NewCodeBlock tip="X86 Linux PC" type="PC">

          ```bash
          vim config_backend.json
          ```

          </NewCodeBlock>

          ```vim
          {
              "graphs": [
                  {
                      "graph_names": [
                          "resnet50"
                      ],
                      "vtcm_mb": 0
                  }
              ],
              "devices": [
                  {
                      "dsp_arch": "v73",
                      "soc_id": 77
                  }
              ]
          }
          ```

      </TabItem>

      </Tabs>

  Here are 4 parameters

  `graph_names`: The list of graph names, which are the names of the unquantized DCL model files (without suffix)

  `vtcm_mb`: A specific memory option, set the maximum VTCM to be used by the device to 0

  `dsp_arch`: NPU architecture

  `soc_id`: SoC id

- config_file.json

    <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  vim config_file.json
  ```

    </NewCodeBlock>

  ```vim
  {
      "backend_extensions": {
          "shared_library_path": "libQnnHtpNetRunExtensions.so",
          "config_file_path": "config_backend.json"
      }
  }
  ```

:::tip
For detailed information on constructing the backend_extensions JSON file, please refer to [**qnn-htp-backend-extensions**](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/htp_backend.html#qnn-htp-backend-extensions)
:::

#### Generate Context-Binary

<Tabs>
    <TabItem value="AIMET">

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    qnn-context-binary-generator --model libQnnModelDlc.so --backend libQnnHtp.so --dlc_path resnet50.dlc --output_dir output --binary_file resnet50_quantized --config_file config_file.json
    ```

    </NewCodeBlock>

    </TabItem>
    <TabItem value="Pretrained ONNX">

    <NewCodeBlock tip="X86 Linux PC" type="PC">

    ```bash
    qnn-context-binary-generator --model libQnnModelDlc.so --backend libQnnHtp.so --dlc_path resnet50_quantized.dlc --output_dir output --binary_file resnet50_quantized --config_file config_file.json
    ```

    </NewCodeBlock>

    </TabItem>

</Tabs>

The generated Context-Binary is saved in `output/resnet50_quantized.bin`

:::tip
For more information on using `qnn-context-binary-generator`, please refer to [**qnn-context-binary-generator**](qairt-tools#qnn-context-binary-generator)
:::

## Model Inference

Using the `qnn-net-run` tool from the QAIRT SDK, you can perform NPU inference of the Context-Binary model on the target board. This tool serves as a testing utility for model inference.

### Clone Example Repository on Target Board

<NewCodeBlock tip="Device" type="device">

```bash
cd ~/
git clone https://github.com/ZIFENG278/resnet50_qairt_example.git
```

</NewCodeBlock>

### Copy Required Files to Target Board

    <Tabs>
        <TabItem value="QCS6490">

        <NewCodeBlock tip="X86 Linux PC" type="PC">
        ```bash
        export PRODUCT_SOC=6490 DSP_ARCH=68
        ```
        </NewCodeBlock>


        </TabItem>

        <TabItem value="QCS9075">

        <NewCodeBlock tip="X86 Linux PC" type="PC">
        ```bash
        export PRODUCT_SOC=9075 DSP_ARCH=73
        ```
        </NewCodeBlock>

    </TabItem>

    </Tabs>

- Copy the Context-Binary model to the target board

    <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  scp resnet50_quantized.bin <user>@<ip address>:/home/<user>/resnet50_qairt_example/model
  ```

    </NewCodeBlock>

- Copy the qnn-net-run executable to the target board

    <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  cd qairt/2.37.1.250807/bin/aarch64-oe-linux-gcc11.2
  scp qnn-net-run <user>@<ip address>:/home/<user>/resnet50_qairt_example/model
  ```

    </NewCodeBlock>

- Copy the required dynamic libraries for qnn-net-run to the target board

    <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  cd qairt/2.37.1.250807/lib/aarch64-oe-linux-gcc11.2
  scp libQnnHtp.so libQnnHtpV${DSP_ARCH}Stub.so <user>@<ip address>:/home/<user>/resnet50_qairt_example/model
  ```

    </NewCodeBlock>

- Copy NPU architecture-specific dynamic library files to the target board

  Select the appropriate hexagon folder based on your SoC NPU architecture. Here we use QCS6490 as an example

    <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  cd qairt/2.37.1.250807/lib/hexagon-v${DSP_ARCH}/unsigned
  scp ./libQnnHtpV${DSP_ARCH}Skel.so  <user>@<ip address>:/home/<user>/resnet50_qairt_example/model
  ```

    </NewCodeBlock>

### On-Device Inference

#### qnn-net-run Inference

- Prepare Test Input Data

  The Context-Binary model requires raw data as input. You need to prepare test raw data for the model input, along with an input data list.

  <NewCodeBlock tip="Device" type="device">

  ```bash
  cd scripts
  python3 create_resnet50_raws.py --dest ../data/test/crop --img_folder ../data/test/ --size 224
  python3 create_file_list.py --input_dir ../data/test/crop/ --output_filename ../model/test_list.txt -e *.raw -r
  ```

  </NewCodeBlock>

- Execute Model Inference

    <NewCodeBlock tip="Device" type="device">

  ```bash
  cd model
  ./qnn-net-run --backend ./libQnnHtp.so --retrieve_context ./resnet50_quantized.bin --input_list ./test_list.txt --output_dir output_bin
  ```

    </NewCodeBlock>

  The results are saved in the `output_bin` directory

  :::tip
  For more information on using `qnn-net-run`, please refer to [**qnn-net-run**](qairt-tools#qnn-net-run)
  :::

#### Result Verification

You can use a Python script to verify the results

<NewCodeBlock tip="Device" type="device">

```bash
cd scripts
python3 show_resnet50_classifications.py --input_list ../model/test_list.txt -o ../model/output_bin/ --labels_file ../data/imagenet_classes.txt
```

</NewCodeBlock>

```bash
$ python3 show_resnet50_classifications.py --input_list ../model/test_list.txt -o ../model/output_bin/ --labels_file ../data/imagenet_classes.txt
Classification results
../data/test/crop/ILSVRC2012_val_00003441.raw 21.740574 402 acoustic guitar
../data/test/crop/ILSVRC2012_val_00008465.raw 23.423716 927 trifle
../data/test/crop/ILSVRC2012_val_00010218.raw 12.623559 281 tabby
../data/test/crop/ILSVRC2012_val_00044076.raw 18.093769 376 proboscis monkey
```

By comparing the printed results with the test images, we can confirm that the resnet50 model has been successfully ported to the Qualcomm® NPU with correct output.

<div style={{ textAlign: "center" }}>
  <img
    src="/en/img/dragon/q6a/qairt_resnet50.webp"
    style={{ width: "60%" }}
    alt="resnet50 input images"
  />
  ResNet50 Input Images
</div>
