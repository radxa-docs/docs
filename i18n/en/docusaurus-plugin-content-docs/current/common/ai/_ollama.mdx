Ollama is an efficient tool for managing and running local large language models (LLMs).
It greatly simplifies AI model deployment. With minimal environment setup, you can pull, run, and manage models on your local device.

## Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

For building from source locally, refer to the [official documentation](https://github.com/ollama/ollama/blob/main/docs/development.md).

## Usage

### Pull a model

This command downloads the model files from the Internet.

```bash
ollama pull deepseek-r1:1.5b
```

### Run a model

This command starts the model. If it is not cached locally, Ollama will download it automatically and then run it.

```bash
ollama run deepseek-r1:1.5b
```

### Show model information

```bash
ollama show deepseek-r1:1.5b
```

### List downloaded models

```bash
ollama list
```

### List loaded models

```bash
ollama ps
```

### Stop a running model

```bash
ollama stop deepseek-r1:1.5b
```

### Remove a model

```bash
ollama rm deepseek-r1:1.5b
```

## References

For more details about Ollama, refer to the [official documentation](https://github.com/ollama/ollama).
