BEV_RoadSeg is a specialized system focused on drivable-area perception for autonomous driving. It combines bird’s-eye-view (BEV) transformation with a Transformer-based architecture, and uses the LSTR deep learning model to segment road structures accurately, enabling stable and reliable detection of lanes and drivable regions in complex, dynamic driving environments.

- Core capability: Generates high-precision BEV segmentation maps of drivable areas and lane lines from multi-camera surround-view inputs, providing key perception signals for path planning.
- Technical highlights: Uses the LSTR model as the core and leverages the Transformer’s strong ability to model long-range spatial relationships, effectively handling challenging scenarios such as curves, intersections, and partial occlusions.
