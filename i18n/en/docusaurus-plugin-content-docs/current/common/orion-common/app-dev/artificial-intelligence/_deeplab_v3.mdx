This document explains how to use the CIX P1 NPU SDK to convert [DeepLabv3](https://pytorch.org/vision/main/models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html) into a model that can run on CIX SOC NPU.

The overall process consists of four steps:
:::tip
Steps 1-3 should be executed in a Linux environment on an x86 host
:::

1. Download the NPU SDK and install NOE Compiler
2. Download model files (code and scripts)
3. Compile the model
4. Deploy the model to Orion O6 / O6N

## Download NPU SDK and Install NOE Compiler

Please refer to [Install NPU SDK](./npu-introduction) for NPU SDK and NOE Compiler installation.

## Download Model Files

The CIX AI Model Hub contains all necessary files for DeepLabv3. Please download them according to the [CIX AI Model Hub](./ai-hub) instructions.

```bash
cd ai_model_hub/models/ComputeVision/Semantic_Segmentation/onnx_deeplab_v3
```

Please verify that the directory structure matches the following:

```bash
.
├── cfg
│   └── onnx_deeplab_v3_build.cfg
├── datasets
│   └── calibration_data.npy
├── graph.json
├── inference_npu.py
├── inference_onnx.py
├── ReadMe.md
├── test_data
│   └── ILSVRC2012_val_00004704.JPEG
└── Tutorials.ipynb
```

## Compile the Model

:::tip
You don't need to compile the model from scratch. Radxa provides a pre-compiled deeplab_v3.cix model (downloadable using the command below). If you use the pre-compiled model, you can skip the "Compile the Model" step.

```bash
wget https://modelscope.cn/models/cix/ai_model_hub_24_Q4/resolve/master/models/ComputeVision/Semantic_Segmentation/onnx_deeplab_v3/deeplab_v3.cix
```

:::

### Prepare ONNX Model

- Download ONNX Model

  [deeplabv3_resnet50.onnx](https://modelscope.cn/models/cix/ai_model_hub_24_Q4/resolve/master/models/ComputeVision/Semantic_Segmentation/onnx_deeplab_v3/model/deeplabv3_resnet50.onnx)

- Simplify the Model

  Use onnxsim for model input shape fixing and model simplification

  ```bash
  pip3 install onnxsim onnxruntime
  onnxsim deeplabv3_resnet50.onnx deeplabv3_resnet50-sim.onnx --overwrite-input-shape 1,3,520,520
  ```

### Compile the Model

CIX SOC NPU supports INT8 computation. Before compiling the model, we need to quantize the model to INT8 using NOE Compiler.

- Prepare Calibration Dataset

  - Use the existing calibration dataset in `datasets`

    ```bash
    .
    └── calibration_data.npy
    ```

  - Or prepare your own calibration dataset

    The `test_data` directory already contains multiple image files for calibration

    ```bash
    .
    ├── 1.jpeg
    └── 2.jpeg
    ```

    Use the following script to generate the calibration file

    ```python
    import sys
    import os
    import numpy as np
    _abs_path = os.path.join(os.getcwd(), "../../../../")
    sys.path.append(_abs_path)
    from utils.image_process import preprocess_image_deeplabv3
    from utils.tools import get_file_list
    # Get a list of images from the provided path
    images_path = "test_data"
    images_list = get_file_list(images_path)
    data = []
    for image_path in images_list:
        input = preprocess_image_deeplabv3(image_path)
        data.append(input)
    # concat the data and save calib dataset
    data = np.concatenate(data, axis=0)
    np.save("datasets/calib_data_tmp.npy", data)
    print("Generate calib dataset success.")
    ```

- Quantize and Compile the Model with NOE Compiler

  - Create a configuration file for quantization and compilation. Refer to the following configuration:

    ```bash
    [Common]
    mode = build

    [Parser]
    model_type = onnx
    model_name = deeplab_v3
    detection_postprocess =
    model_domain = image_segmentation
    input_model = ./deeplabv3_resnet50-sim.onnx
    input = input
    input_shape = [1, 3, 520, 520]
    output = output
    output_dir = ./

    [Optimizer]
    output_dir = ./
    calibration_data = ./datasets/calib_data_tmp.npy
    calibration_batch_size = 1
    metric_batch_size = 1
    dataset = NumpyDataset
    quantize_method_for_weight = per_channel_symmetric_restricted_range
    quantize_method_for_activation = per_tensor_asymmetric
    save_statistic_info = True

    [GBuilder]
    outputs = deeplab_v3.cix
    target = X2_1204MP3
    profile = True
    tiling = fps
    ```

  - Compile the Model
    :::tip
    If you encounter the cixbuild error: `[E] Optimizing model failed! CUDA error: no kernel image is available for execution on the device ...`
    This means the current version of PyTorch doesn't support your GPU. Please completely uninstall the current PyTorch version and download the latest version from the official PyTorch website.
    :::
    ```bash
    cixbuild ./onnx_deeplab_v3_build.cfg
    ```

## Model Deployment

### NPU Inference

Copy the compiled .cix model file to your Orion O6 / O6N development board for model validation:

```bash
python3 inference_npu.py --images ./test_data/ --model_path ./deeplab_v3.ci
```

Example output:

```bash
(.venv) radxa@orion-o6:~/NOE/ai_model_hub/models/ComputeVision/Semantic_Segmentation/onnx_deeplab_v3$ time python3 inference_npu.py --images ./test_data/ --model_path ./deeplab_v3.cix
npu: noe_init_context success
npu: noe_load_graph success
Input tensor count is 1.
Output tensor count is 1.
npu: noe_create_job success
save output: noe_ILSVRC2012_val_00004704.JPEG
npu: noe_clean_job success
npu: noe_unload_graph success
npu: noe_deinit_context success

real	0m9.047s
user	0m4.314s
sys	0m0.478s
```

The results are saved in the `output` directory.

![deeplab1.webp](/img/orion/o6/deeplab1.webp)

### CPU Inference

Run inference on the ONNX model using CPU for verification. This can be executed on either an x86 host or Orion O6 / O6N:

```bash
python3 inference_onnx.py --images ./test_data/ --onnx_path ./deeplabv3_resnet50-sim.onnx
```

Example output:

```bash
(.venv) radxa@orion-o6:~/NOE/ai_model_hub/models/ComputeVision/Semantic_Segmentation/onnx_deeplab_v3$ time python3 inference_onnx.py --images ./test_data/ --onnx_path ./deeplabv3_resnet50-sim.onnx
save output: onnx_ILSVRC2012_val_00004704.JPEG

real	0m7.605s
user	0m33.235s
sys	0m0.558s
```

The results are saved in the `output` directory.
![deeplab2.webp](/img/orion/o6/deeplab2.webp)

You can see that the inference results are consistent between NPU and CPU, but the NPU provides significantly faster execution speed.

## References

Paper: [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)
