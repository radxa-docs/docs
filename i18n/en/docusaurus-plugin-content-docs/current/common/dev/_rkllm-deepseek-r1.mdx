[DeepSeek-R1](https://api-docs.deepseek.com/news/news250120) is a state-of-the-art reasoning model developed by DeepSeek.
DeepSeek has open-sourced the training approach and model weights, and its performance is competitive with closed-source reasoning models.
DeepSeek also released multiple distilled open-source lightweight variants (covering the Qwen2.5 and Llama3.1 families) using knowledge distillation.
This document demonstrates how to deploy the distilled **DeepSeek-R1-Distill-Qwen-1.5B** model to an RK3588 device with the RKLLM toolchain and run hardware-accelerated inference on the built-in NPU.

![rkllm_2.webp](/img/general-tutorial/rknn/rkllm_ds_1.webp)

## Quick Start

### Download the demo

Download the complete demo from ModelScope.

<NewCodeBlock tip="Device" type="device">

```bash
pip install -U modelscope
modelscope download --model radxa/DeepSeek-R1-Distill-Qwen-1.5B_RKLLM
```

</NewCodeBlock>

### Run the Example

<NewCodeBlock tip="Device" type="device">

```bash
cd demo_Linux_aarch64/
export LD_LIBRARY_PATH=./lib
./llm_demo ../DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm 2048 4096
```

</NewCodeBlock>

## Full Conversion Workflow

:::info[Prerequisites]
Set up the development environment by following [RKLLM Installation](./rkllm-install).
:::

:::warning[Version note]
Running this example with RKLLM **1.2.3** may cause severe quality degradation (repetitive output).
It is recommended to use RKLLM **1.2.2** for this demo. See: [GitHub Issue](https://github.com/airockchip/rknn-llm/issues/424).
:::

### Activate the virtual environment

<NewCodeBlock tip="X64 Linux PC" type="PC">

```bash
conda activate rkllm
pip install -U huggingface_hub
```

</NewCodeBlock>

### Download the Model

<NewCodeBlock tip="X64 Linux PC" type="PC">

```bash
cd RK-SDK/rknn-llm/examples/rkllm_api_demo/
hf download https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --local-dir ./DeepSeek-R1-Distill-Qwen-1.5B
```

</NewCodeBlock>

### Model Conversion

Generate a quantization calibration file and export the model to the RKLLM format.

:::tip
If you need a different `max_context` length, adjust the `max_context` parameter in the `llm.build` call in `export_rkllm.py`.
The default is **4096**. Larger values use more memory. The value must be ≤ **16384** and a multiple of **32** (e.g., 32, 64, 96, …, 16384).
:::

<NewCodeBlock tip="X64 Linux PC" type="PC">

```bash
cd export/
python generate_data_quant.py -m ../DeepSeek-R1-Distill-Qwen-1.5B -o ../DeepSeek-R1-Distill-Qwen-1.5B/data_quant.json
# Before running, update the model path and calibration file path as needed.
python export_rkllm.py
```

</NewCodeBlock>

### Build the executable

<NewCodeBlock tip="X64 Linux PC" type="PC">

```bash
cd ../deploy/
# Export the cross-compiler path.
export GCC_COMPILER=/path/to/your/gcc/bin/aarch64-linux-gnu
bash build-linux.sh
```

</NewCodeBlock>

The generated binaries are located at `install/demo_Linux_aarch64`.

### Deploy to the device

Copy the converted model and the built `demo_Linux_aarch64` directory to the device.

<NewCodeBlock tip="Device" type="device">

```bash
cd demo_Linux_aarch64/
export RKLLM_LOG_LEVEL=1
export LD_LIBRARY_PATH=./lib
./llm_demo ../DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm 2048 4096
```

</NewCodeBlock>

Run the demo. Type `exit` to quit.

<NewCodeBlock tip="Device" type="device">

```bash
./llm_demo ../DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm 2048 4096
```

</NewCodeBlock>

```bash
$ ./llm_demo ../DeepSeek-R1-Distill-Qwen-1.5B_W8A8_RK3588.rkllm 2048 4096
rkllm init start
I rkllm: rkllm-runtime version: 1.2.2, rknpu driver version: 0.9.8, platform: RK3588
...
rkllm init success

user: Solve x+y=14 and 2x+4y=38.
assistant: x=9, y=5
```

| Parameter         | Required | Description               | Notes                          |
| ----------------- | -------- | ------------------------- | ------------------------------ |
| `path`            | Yes      | Path to the RKLLM model   | N/A                            |
| `max_new_tokens`  | Yes      | Max generated tokens/turn | Must be ≤ `max_context_len`    |
| `max_context_len` | Yes      | Max context length        | Must be ≤ export `max_context` |

### Performance

For the math prompt: `Solve x+y=12 and 2x+4y=34. Find x and y.`,

RK3588 achieves **15.36 tokens/s**:

| Stage    | Total Time (ms) | Tokens | Time per Token (ms) | Tokens per Second |
| -------- | --------------- | ------ | ------------------- | ----------------- |
| Prefill  | 122.70          | 29     | 4.23                | 236.35            |
| Generate | 27539.16        | 423    | 65.10               | 15.36             |

RK3582 achieves **10.61 tokens/s**:
| Stage | Total Time (ms) | Tokens | Time per Token (ms) | Tokens per Second |
|----------|-----------------|--------|---------------------|-------------------|
| Prefill | 599.71 | 81 | 7.4 | 135.07 |
| Generate | 76866.41 | 851 | 94.25 | 10.61 |
