This document explains how to use RKLLM to deploy Hugging Face-format LLMs to RK3588 and run hardware-accelerated inference on the NPU.

#### Supported models

- [LLAMA models](https://huggingface.co/meta-llama)
- [TinyLLAMA models](https://huggingface.co/TinyLlama)
- [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)
- [Phi models](https://huggingface.co/models?search=microsoft/phi)
- [ChatGLM3-6B](https://huggingface.co/THUDM/chatglm3-6b/tree/103caa40027ebfd8450289ca2f278eac4ff26405)
- [Gemma2](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)
- [Gemma3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
- [InternLM2 models](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)
- [MiniCPM models](https://huggingface.co/collections/openbmb/minicpm-65d48bf958302b9fd25b698f)
- [TeleChat models](https://huggingface.co/Tele-AI)
- [Qwen2-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)
- [MiniCPM-V-2_6](https://huggingface.co/openbmb/MiniCPM-V-2_6)
- [DeepSeek-R1-Distill](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d)
- [Janus-Pro-1B](https://huggingface.co/deepseek-ai/Janus-Pro-1B)
- [InternVL2-1B](https://huggingface.co/OpenGVLab/InternVL2-1B)
- [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)

This guide uses [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) as an example and follows the demo scripts in the [RKLLM](./rkllm_install) repository to walk through an end-to-end deployment on an RK3588 device with NPU acceleration.
:::tip
If you haven't installed and configured RKLLM yet, follow [RKLLM Installation](rkllm_install).
:::

### Model Conversion

:::tip
For RK358x, set `TARGET_PLATFORM` to `rk3588`.
:::

This section uses [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) as an example. You can also pick any model from the [Supported models](#supported-models) list.

- On an x86 Linux PC, download the model weights (install [git-lfs](https://git-lfs.com/) if needed):

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  git lfs install
  git clone https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct
  ```

  </NewCodeBlock>

- Activate the `rkllm` conda environment (see [RKLLM conda installation](rkllm_install)):

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  conda activate rkllm
  ```

  </NewCodeBlock>

- Generate the quantization calibration file for the LLM
  :::tip
  For LLM models, this guide uses the conversion scripts under `rknn-llm/xamples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/export`.

  For VLM models, use `rknn-llm/examples/Qwen2-VL_Demo/export`. For multimodal VLM models, see [RKLLM Qwen2-VL](./rkllm_qwen2_vl).
  :::

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  cd examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/export
  python3 generate_data_quant.py -m /path/to/Qwen2.5-1.5B-Instruct
  ```

  </NewCodeBlock>

  | Parameter | Required | Description                  | Notes |
  | --------- | -------- | ---------------------------- | ----- |
  | `path`    | Yes      | Hugging Face model directory | N/A   |

  `generate_data_quant.py` generates `data_quant.json`, which is used during quantization.

- Update the `modelpath` in `rknn-llm/xamples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/export/export_rkllm.py`:

  <NewCodeBlock tip="Python Code" type="PC">

  ```python
  11 modelpath = '/path/to/Qwen2.5-1.5B-Instruct'
  ```

  </NewCodeBlock>

- Adjust `max_context` (optional)

  If you need a different context length, modify `max_context` in the `llm.build` call in `rknn-llm/xamples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/export/export_rkllm.py`.
  Default is **4096**. Larger values consume more memory. The value must be ≤ **16384** and a multiple of **32** (e.g., 32, 64, 96, …, 16384).

- Run the conversion script

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  python3 export_rkllm.py
  ```

  </NewCodeBlock>

  After a successful conversion, you should get an RKLLM model such as `Qwen2.5-1.5B-Instruct_W8A8_RK3588.rkllm`.
  The name indicates this model is W8A8-quantized and targeted for RK3588.

### Build the executable

- Download the cross-compilation toolchain: [gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu](https://developer.arm.com/downloads/-/gnu-a/10-2-2020-11)
- Update the main program: `rknn-llm/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/deploy/src/llm_demo.cpp`

  Comment out line 165. RKLLM parses the `chat_template` from `tokenizer_config.json` automatically during conversion, so you don't need to set it manually.

  <NewCodeBlock tip="CPP Code" type="PC">

  ```vim
  165 // rkllm_set_chat_template(llmHandle, "", "<｜User｜>", "<｜Assistant｜>");
  ```

  </NewCodeBlock>

- Update `GCC_COMPILER_PATH` in `rknn-llm/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/deploy/build-linux.sh`

  <NewCodeBlock tip="BASH" type="PC">

  ```vim
  8 GCC_COMPILER_PATH=/path/to/gcc-arm-10.2-2020.11-x86_64-aarch64-none-linux-gnu/bin/aarch64-none-linux-gnu
  ```

  </NewCodeBlock>

- Build

  <NewCodeBlock tip="X86 Linux PC" type="PC">

  ```bash
  cd rknn-llm/examples/DeepSeek-R1-Distill-Qwen-1.5B_Demo/deploy/
  bash build-linux.sh
  ```

  </NewCodeBlock>

  The generated binaries are located at `install/demo_Linux_aarch64`.

### Deploy to the device

#### Local terminal mode

- Copy the converted RKLLM model and the built `demo_Linux_aarch64` folder to the device.
- Export environment variables:

  <NewCodeBlock tip="Radxa OS" type="device">

  ```bash
  export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/path/to/demo_Linux_aarch64/lib
  ```

  </NewCodeBlock>

- Run `llm_demo` (type `exit` to quit):

  <NewCodeBlock tip="Radxa OS" type="device">

  ```bash
  export RKLLM_LOG_LEVEL=1
  ## Usage: ./llm_demo model_path max_new_tokens max_context_len
  ./llm_demo /path/to/Qwen2.5-1.5B-Instruct_W8A8_RK3588.rkllm 2048 4096
  ```

  </NewCodeBlock>

  | Parameter         | Required | Description               | Notes                          |
  | ----------------- | -------- | ------------------------- | ------------------------------ |
  | `path`            | Yes      | Path to the RKLLM model   | N/A                            |
  | `max_new_tokens`  | Yes      | Max generated tokens/turn | Must be ≤ `max_context_len`    |
  | `max_context_len` | Yes      | Max context length        | Must be ≤ export `max_context` |

  ![rkllm_2.webp](/img/general-tutorial/rknn/rkllm_2.webp)

### Performance comparison (selected models)

| Model     | Parameter Size | Chip   | Chip Count | Inference Speed |
| --------- | -------------- | ------ | ---------- | --------------- |
| TinyLlama | 1.1B           | RK3588 | 1          | 15.03 token/s   |
| Qwen      | 1.8B           | RK3588 | 1          | 14.18 token/s   |
| Phi3      | 3.8B           | RK3588 | 1          | 6.46 token/s    |
| ChatGLM3  | 6B             | RK3588 | 1          | 3.67 token/s    |
| Qwen2.5   | 1.5B           | RK3588 | 1          | 15.44 token/s   |
